{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "826621b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   fixture_id     season season_start_date season_end_date country league  \\\n",
      "0     1320059  2014-2015        2014-08-01      2015-05-22  France  FraL2   \n",
      "1     1320060  2014-2015        2014-08-01      2015-05-22  France  FraL2   \n",
      "2     1320061  2014-2015        2014-08-01      2015-05-22  France  FraL2   \n",
      "3     1320062  2014-2015        2014-08-01      2015-05-22  France  FraL2   \n",
      "4     1320063  2014-2015        2014-08-01      2015-05-22  France  FraL2   \n",
      "\n",
      "   competition_level     kick_off_datetime       team1_name    team2_name  \\\n",
      "0                  2  2014-08-01T18:00:00Z            Arles       Ajaccio   \n",
      "1                  2  2014-08-01T18:00:00Z       AJ Auxerre      Le Havre   \n",
      "2                  2  2014-08-04T18:30:00Z            Brest      Clermont   \n",
      "3                  2  2014-08-01T18:00:00Z  Gazelec Ajaccio  Valenciennes   \n",
      "4                  2  2014-08-01T18:00:00Z      Chateauroux        Troyes   \n",
      "\n",
      "   ...  stadium_surface  stadium_runningtrack  stadium_capacity  \\\n",
      "0  ...            grass                     0             17518   \n",
      "1  ...            grass                     0             23467   \n",
      "2  ...            grass                     0             15931   \n",
      "3  ...            grass                     0              4045   \n",
      "4  ...  artificial turf                     0             17072   \n",
      "\n",
      "   stadium_altitude  stadium_country  distance  team1_stadium_dist  \\\n",
      "0                25           France     390.0                 0.0   \n",
      "1                90           France     315.4                 0.0   \n",
      "2                88           France     642.3                 0.0   \n",
      "3                63           France    1017.7                 0.0   \n",
      "4               155           France     244.9                 0.0   \n",
      "\n",
      "   team2_stadium_dist  indoor_outdoor  ave_dist  \n",
      "0               390.0         Outdoor  6.082074  \n",
      "1               315.4         Outdoor  6.082074  \n",
      "2               642.3         Outdoor  6.082074  \n",
      "3              1017.7         Outdoor  6.082074  \n",
      "4               244.9         Outdoor  6.082074  \n",
      "\n",
      "[5 rows x 34 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n",
    "\n",
    "from Data_Setup_Main import prepare_data\n",
    "X, y, preprocessor = prepare_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "45c27220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42\n",
    ")\n",
    "\n",
    "# Pipeline: preprocess -> decision tree\n",
    "pipe = Pipeline(steps=[\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"reg\", DecisionTreeRegressor(\n",
    "        max_depth=7,          \n",
    "        min_samples_leaf=10, \n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Fit\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = pipe.predict(X_test)\n",
    "# Clip negatives to 0\n",
    "y_pred = np.clip(y_pred, 0, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "081265b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Per-target metrics ===\n",
      "     target     MSE     MAE    RMSE      R2\n",
      "0  team1_yc  1.4964  0.9957  1.2233 -0.0177\n",
      "1  team2_yc  1.5991  1.0087  1.2646 -0.0246\n",
      "2  team1_rc  0.0470  0.0990  0.2167 -0.0269\n",
      "3  team2_rc  0.0795  0.1452  0.2820 -0.0299\n"
     ]
    }
   ],
   "source": [
    "targets = y.columns.tolist()\n",
    "rows = []\n",
    "for i, name in enumerate(targets):\n",
    "    ytest = y_test.iloc[:, i].values\n",
    "    ypred = y_pred[:, i]\n",
    "\n",
    "    mse  = mean_squared_error(ytest, ypred)\n",
    "    mae  = mean_absolute_error(ytest, ypred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2   = r2_score(ytest, ypred)\n",
    "\n",
    "    rows.append({\"target\": name, \"MSE\": mse, \"MAE\": mae, \"RMSE\": rmse, \"R2\": r2})\n",
    "\n",
    "metrics_df = pd.DataFrame(rows)\n",
    "\n",
    "print(\"\\n=== Per-target metrics ===\")\n",
    "print(metrics_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "54472ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample predictions (first 5 rows, order: ['team1_yc', 'team2_yc', 'team1_rc', 'team2_rc'] )\n",
      "[[1.607 1.953 0.069 0.052]\n",
      " [1.374 1.388 0.054 0.068]\n",
      " [1.778 1.222 0.111 0.056]\n",
      " [1.333 0.5   0.    0.083]\n",
      " [1.441 1.75  0.038 0.088]]\n"
     ]
    }
   ],
   "source": [
    "# Quick peek at first 5 predictions for sanity check\n",
    "print(\"\\nSample predictions (first 5 rows, order:\", targets, \")\")\n",
    "print(np.round(y_pred[:5], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "27468798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Weights used (sum to 1) ===\n",
      "team1_yc    0.4662\n",
      "team2_yc    0.4948\n",
      "team1_rc    0.0145\n",
      "team2_rc    0.0245\n",
      "dtype: float64\n",
      "\n",
      "=== Weighted average across targets ===\n",
      "MSE     1.4915\n",
      "MAE     0.9683\n",
      "RMSE    1.2061\n",
      "R2     -0.0216\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "w = y_test.var(axis=0, ddof=0)   # variance per target (Series aligned with columns)\n",
    "\n",
    "weights = w / w.sum()\n",
    "\n",
    "print(\"\\n=== Weights used (sum to 1) ===\")\n",
    "print(weights.round(4))\n",
    "\n",
    "# compute weighted averages\n",
    "weighted_mse  = np.average(metrics_df[\"MSE\"].values,  weights=weights.values)\n",
    "weighted_mae  = np.average(metrics_df[\"MAE\"].values,  weights=weights.values)\n",
    "weighted_rmse = np.average(metrics_df[\"RMSE\"].values, weights=weights.values)\n",
    "weighted_r2   = np.average(metrics_df[\"R2\"].values,   weights=weights.values)\n",
    "\n",
    "print(\"\\n=== Weighted average across targets ===\")\n",
    "print(pd.Series({\n",
    "    \"MSE\":  weighted_mse,\n",
    "    \"MAE\":  weighted_mae,\n",
    "    \"RMSE\": weighted_rmse,\n",
    "    \"R2\":   weighted_r2\n",
    "}).round(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8c7e6c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== True vs Predicted (first 10 rows) ===\n",
      "   team1_yc  team2_yc  team1_rc  team2_rc  team1_yc_pred  team2_yc_pred  \\\n",
      "0         6         2         0         0              2              2   \n",
      "1         0         0         0         0              1              1   \n",
      "2         2         2         0         0              2              1   \n",
      "3         1         1         0         0              1              0   \n",
      "4         3         1         0         0              1              2   \n",
      "5         0         1         0         0              0              2   \n",
      "6         2         5         0         0              2              2   \n",
      "7         2         1         0         0              2              2   \n",
      "8         0         0         0         0              1              1   \n",
      "9         2         5         0         0              2              2   \n",
      "\n",
      "   team1_rc_pred  team2_rc_pred  \n",
      "0              0              0  \n",
      "1              0              0  \n",
      "2              0              0  \n",
      "3              0              0  \n",
      "4              0              0  \n",
      "5              0              0  \n",
      "6              0              0  \n",
      "7              0              0  \n",
      "8              0              0  \n",
      "9              0              0  \n"
     ]
    }
   ],
   "source": [
    "# True vs Predicted side-by-side (first 10 rows) \n",
    "y_pred_rounded = np.rint(y_pred).clip(min=0).astype(int)\n",
    "\n",
    "y_pred_df = pd.DataFrame(y_pred_rounded, columns=[f\"{c}_pred\" for c in targets], index=y_test.index)\n",
    "\n",
    "comparison = pd.concat([y_test.reset_index(drop=True), y_pred_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "print(\"\\n=== True vs Predicted (first 10 rows) ===\")\n",
    "print(comparison.head(10).round(2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
